\relax 
\providecommand\zref@newlabel[2]{}
\bibstyle{biblatex}
\bibdata{main-blx,bibliography}
\citation{biblatex-control}
\abx@aux@refcontext{none/global//global/global}
\citation{Zeng-survey}
\abx@aux@cite{Zeng-survey}
\abx@aux@segm{0}{0}{Zeng-survey}
\citation{8376254}
\abx@aux@cite{8376254}
\abx@aux@segm{0}{0}{8376254}
\citation{wei2013}
\abx@aux@cite{wei2013}
\abx@aux@segm{0}{0}{wei2013}
\citation{7903611}
\abx@aux@cite{7903611}
\abx@aux@segm{0}{0}{7903611}
\citation{quaglia}
\abx@aux@cite{quaglia}
\abx@aux@segm{0}{0}{quaglia}
\citation{li2010security}
\abx@aux@cite{li2010security}
\abx@aux@segm{0}{0}{li2010security}
\citation{7270404}
\abx@aux@cite{7270404}
\abx@aux@segm{0}{0}{7270404}
\citation{Baracca-12}
\abx@aux@cite{Baracca-12}
\abx@aux@segm{0}{0}{Baracca-12}
\citation{7398138}
\abx@aux@cite{7398138}
\abx@aux@segm{0}{0}{7398138}
\citation{Zeng-survey}
\abx@aux@segm{0}{0}{Zeng-survey}
\citation{Brands}
\abx@aux@cite{Brands}
\abx@aux@segm{0}{0}{Brands}
\citation{singelee2005location}
\abx@aux@cite{singelee2005location}
\abx@aux@segm{0}{0}{singelee2005location}
\citation{song2008secure}
\abx@aux@cite{song2008secure}
\abx@aux@segm{0}{0}{song2008secure}
\citation{Sastry}
\abx@aux@cite{Sastry}
\abx@aux@segm{0}{0}{Sastry}
\citation{Vora}
\abx@aux@cite{Vora}
\abx@aux@segm{0}{0}{Vora}
\citation{7145434}
\abx@aux@cite{7145434}
\abx@aux@segm{0}{0}{7145434}
\citation{Brands}
\abx@aux@segm{0}{0}{Brands}
\citation{Sastry}
\abx@aux@segm{0}{0}{Sastry}
\citation{Vora}
\abx@aux@segm{0}{0}{Vora}
\citation{quaglia}
\abx@aux@segm{0}{0}{quaglia}
\citation{singelee2005location}
\abx@aux@segm{0}{0}{singelee2005location}
\citation{song2008secure}
\abx@aux@segm{0}{0}{song2008secure}
\citation{Sastry}
\abx@aux@segm{0}{0}{Sastry}
\citation{Vora}
\abx@aux@segm{0}{0}{Vora}
\citation{yan2016location}
\abx@aux@cite{yan2016location}
\abx@aux@segm{0}{0}{yan2016location}
\citation{li2010security}
\abx@aux@segm{0}{0}{li2010security}
\citation{Cover-book}
\abx@aux@cite{Cover-book}
\abx@aux@segm{0}{0}{Cover-book}
\citation{xiao-2018}
\abx@aux@cite{xiao-2018}
\abx@aux@segm{0}{0}{xiao-2018}
\citation{tian2015robust}
\abx@aux@cite{tian2015robust}
\abx@aux@segm{0}{0}{tian2015robust}
\citation{xiao-2018}
\abx@aux@segm{0}{0}{xiao-2018}
\citation{tian2015robust}
\abx@aux@segm{0}{0}{tian2015robust}
\providecommand \oddpage@label [2]{}
\providecommand\@newglossary[4]{}
\@newglossary{main}{glg}{gls}{glo}
\@newglossary{acronym}{alg}{acr}{acn}
\providecommand\@glsorder[1]{}
\providecommand\@istfilename[1]{}
\@istfilename{main.ist}
\@glsorder{word}
\@writefile{toc}{\boolfalse {citerequest}\boolfalse {citetracker}\boolfalse {pagetracker}\boolfalse {backtracker}\relax }
\@writefile{lof}{\boolfalse {citerequest}\boolfalse {citetracker}\boolfalse {pagetracker}\boolfalse {backtracker}\relax }
\@writefile{lot}{\boolfalse {citerequest}\boolfalse {citetracker}\boolfalse {pagetracker}\boolfalse {backtracker}\relax }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {I}Introduction}{1}}
\newlabel{sec:intro}{{I}{1}}
\newlabel{rev2lit}{{1}{1}}
\zref@newlabel{rev2lit}{\default{1}\page{1}\revcontent{Location verification can be classified into two main sub-problems: \emph  {single location verification} and {\it  \ac {irlv}}. The \emph  {single location verification} problem aims at verifying if a user is in a specific point. A solution is obtained by comparing some channel features of the user under test with those of a trusted user that was in the same location in the past. }\revsec{I}}
\newlabel{rev3cit}{{2}{1}}
\zref@newlabel{rev3cit}{\default{2}\page{1}\revcontent{In some works, this approach is used to verify if different messages come from the same user, i.e., as a {\it  user authentication} mechanism (see {\cite {7270404}} for a survey): in {\cite {Baracca-12}}, channel features are affected by noise with known statistics; whereas, in {\cite {7398138}}, statistics are unknown and a learning strategy is adopted. The {\it  \ac {irlv}} aims at verifying if a user is inside a \ac {roi} \cite {Zeng-survey}. }\revsec{I}}
\newlabel{rev2lit2}{{3}{1}}
\zref@newlabel{rev2lit2}{\default{3}\page{1}\revcontent{Solutions include distance bounding techniques with rapid exchanges of packets between the verifier and the prover \cite {Brands, singelee2005location}, also in the context of vehicular ad-hoc networks {\cite {song2008secure}}. Other solutions use radio-frequency and ultrasound signals {\cite {Sastry}}, or anchor nodes and transmit power variations {\cite {Vora}}. More recently, a delay-based verification technique leveraging geometric properties has been proposed in {\cite {7145434}}. Some of the proposed techniques partially neglect wireless propagation phenomena (such as shadowing and fading) that corrupt the distance estimates {\cite {Brands,Sastry} and \cite {Vora}}. Other approaches assume specific channel statistics that may be not accurate due to changing environment conditions {\cite {quaglia}}.}\revsec{I}}
\newlabel{attack1}{{4}{1}}
\zref@newlabel{attack1}{\default{4}\page{1}\revcontent{Two types of attacks to \ac {irlv} have been considered in the literature, where the attacker claims a false location \unhbox \voidb@x \hbox {\cite {singelee2005location,song2008secure,Sastry}} or tampers with the signal power making it coherent with the fake claimed position \unhbox \voidb@x \hbox {\cite {Vora,yan2016location}} and \unhbox \voidb@x \hbox {\cite {li2010security}}.}\revsec{I}}
\newlabel{rev11a}{{5}{1}}
\zref@newlabel{rev11a}{\default{5}\page{1}\revcontent{The obtained asymptotic results are applicable also to elaborate ML solutions, such as deep learning NNs, that can still be seen as parametric functions, although more complex than shallow NNs.}\revsec{I}}
\citation{Vora}
\abx@aux@segm{0}{0}{Vora}
\citation{Sastry}
\abx@aux@segm{0}{0}{Sastry}
\citation{singelee2005location}
\abx@aux@segm{0}{0}{singelee2005location}
\citation{song2008secure}
\abx@aux@segm{0}{0}{song2008secure}
\citation{li2010security}
\abx@aux@segm{0}{0}{li2010security}
\citation{yan2016location}
\abx@aux@segm{0}{0}{yan2016location}
\citation{Zeng-survey}
\abx@aux@segm{0}{0}{Zeng-survey}
\citation{li2010security}
\abx@aux@segm{0}{0}{li2010security}
\citation{Vora}
\abx@aux@segm{0}{0}{Vora}
\citation{yan2016location}
\abx@aux@segm{0}{0}{yan2016location}
\citation{7270404}
\abx@aux@segm{0}{0}{7270404}
\citation{3gpp}
\abx@aux@cite{3gpp}
\abx@aux@segm{0}{0}{3gpp}
\citation{goldsmith2005}
\abx@aux@cite{goldsmith2005}
\abx@aux@segm{0}{0}{goldsmith2005}
\citation{3gpp}
\abx@aux@segm{0}{0}{3gpp}
\newlabel{lit2}{{6}{2}}
\zref@newlabel{lit2}{\default{6}\page{2}\revcontent{About point 1, shadowing and fading effects on \ac {irlv} have not been much considered in the literature: for example, in {\cite {Vora},} \ac {rss} estimates are assumed to be perfect; in {\cite {Sastry}}, agents are assumed to communicate over an error-free channel (san assumption used for most distance-bounding protocols {\cite {singelee2005location,song2008secure}}). In {\cite {li2010security}} and {\cite {yan2016location}}, shadowing is taken into account, while fading is neglected, and channel statistics is assumed to be known. All these simplifying assumptions are not required by the \ac {ml} models studied in this paper.}\revsec{I}}
\newlabel{framework1}{{7}{2}}
\zref@newlabel{framework1}{\default{7}\page{2}\revcontent{Indeed, we also consider an accurate wireless channel model (in Section II), but only in order to explain the complexity of the techniques in the literature (including the \ac {np} test) and, consequently, justify the use of \ac {ml}. Still, our solution and theoretical results can be applied on any channel statistics and various features (see \cite {Zeng-survey} for a survey), even including measurements from external sensors.}\revsec{I}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {II}System Model}{2}}
\newlabel{revPHASE}{{8}{2}}
\zref@newlabel{revPHASE}{\default{8}\page{2}\revcontent{For example, we consider as feature the channel power attenuation (of a narrowband transmission), similarly to {\cite {li2010security,Vora}} and {\cite {yan2016location}}. Indeed, other features can be exploited, such as the phase or the wideband impulse response (see {\cite {7270404}} for a survey): our solutions readily apply also to these cases, as we do not make special assumptions on the channel model for their design and analysis.}\revsec{II}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {II-A}}Channel Model}{2}}
\newlabel{sec:chMod}{{\unhbox \voidb@x \hbox {II-A}}{2}}
\newlabel{WiFi2}{{9}{2}}
\zref@newlabel{WiFi2}{\default{9}\page{2}\revcontent{In particular, we consider the general channel \cite {3gpp} model that covers a large frequency range from $800$\nobreakspace  {}MHz to 2.5\nobreakspace  {}GHz, suitable for wireless local area networks (WLANs) and IoT, where \ac {irlv} is typically applied.}\revsec{II}}
\newlabel{eq:los}{{3}{2}}
\citation{Kay-book}
\abx@aux@cite{Kay-book}
\abx@aux@segm{0}{0}{Kay-book}
\citation{li2010security}
\abx@aux@segm{0}{0}{li2010security}
\newlabel{eq:nlos}{{4}{3}}
\newlabel{avg_1}{{10}{3}}
\zref@newlabel{avg_1}{\default{10}\page{3}\revcontent{Fading does not give information on the \ac {ue} location; therefore it is a disturbance for \ac {irlv}. However, by performing $k_f$ estimates of the attenuation in a short time $a_j^{(n)}$, $j=1, \ldots  ,k_f$, and averaging them, we obtain the new attenuation estimate}\revsec{II}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {II-B}}\ac {irlv} With Known Channel Statistics}{3}}
\newlabel{sec:auth}{{\unhbox \voidb@x \hbox {II-B}}{3}}
\newlabel{eq:lr}{{6}{3}}
\newlabel{eq:oneClassDec}{{7}{3}}
\newlabel{lambda}{{11}{3}}
\zref@newlabel{lambda}{\default{11}\page{3}\revcontent{Parameter $\Lambda $ must be chosen to obtain a desired significance level, i.e., a desired \ac {fa} probability. It can be set either by assessing the \ac {fa} probability through simulations or by inverting, when available, the expression of \ac {fa} probability as a function of $\Lambda $ {\cite [Section 3.3]{Kay-book}}.}\revsec{II}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Simplified scenario with a single \ac {ap} located at the center of a circular \ac {roi}.}}{3}}
\newlabel{fig:simpScen}{{1}{3}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {II-C}}Example of \ac {np} Test}{3}}
\newlabel{sec:los}{{\unhbox \voidb@x \hbox {II-C}}{3}}
\newlabel{simpleScen}{{12}{3}}
\zref@newlabel{simpleScen}{\default{12}\page{3}\revcontent{As example of application of the \ac {np} test we consider the scenario of Fig. 1\hbox {}, wherein area $\mathcal  {A}$ is a ring with smaller radius $R_{\rm  min}$ and larger radius $R_{\rm  out}$ and \ac {roi} $\mathcal  {A}_{0}$ is a ring concentric to $\mathcal  {A}$, with larger radius $R_{\rm  in}$ and smaller radius $R_{\rm  min}$. A single \ac {ap} ($N_{\rm  AP} =1$) is located at the \ac {roi} center and a \ac {ue} is transmitting from distance $d_0$. We consider two models: a) \emph  {uncorrelated fading scenario}, which includes \ac {los} path-loss and spatially uncorrelated fading, and b) \emph  {uncorrelated shadowing scenario}, which includes \ac {los} path loss and spatially uncorrelated shadowing. In both case, we consider the \ac {los} model for path-loss.}\revsec{II}}
\newlabel{simpleScen2}{{13}{3}}
\zref@newlabel{simpleScen2}{\default{13}\page{3}\revcontent{In order to compute}\revsec{II}}
\newlabel{simpleScen2_0}{{14}{3}}
\zref@newlabel{simpleScen2_0}{\default{14}\page{3}\revcontent{$p(a|\mathcal  {H}_i)$, we first observe that the \ac {pdf} of incurring in attenuation $a$ for a user located inside the \ac {roi} is (by the total probability law)}\revsec{II}}
\newlabel{eq:prc}{{8}{3}}
\newlabel{simpleScen2_1}{{15}{3}}
\zref@newlabel{simpleScen2_1}{\default{15}\page{3}\revcontent{where $p(d_0| d_0 \in \mathcal  {A}_0)$ is the \ac {pdf} of the \ac {ue} transmitting from distance $d_0$ inside the \ac {roi}. Assuming that \ac {ue} position is uniformly distributed in $\mathcal  {A}$, and letting $\Delta _0 = R_{\rm  in}^2-R_{\rm  min}^2$ and $\Delta _1= R_{\rm  out}^2-R_{\rm  in}^2$, we have $p(d_0| d_0 \in \mathcal  {A}_0) = \frac  {2 d_0}{\Delta _0}$ for $d_0 \in [R_{\rm  min}, R_{\rm  in}]$, and $p(d_0| d_0 \in \mathcal  {A}_0) = 0$ otherwise.}\revsec{II}}
\newlabel{simpleScen2_2}{{16}{3}}
\zref@newlabel{simpleScen2_2}{\default{16}\page{3}\revcontent{A similar expression holds for $p(d_0|d_0 \in \mathcal  {A}_1)$.}\revsec{II}}
\newlabel{llrComp}{{17}{3}}
\zref@newlabel{llrComp}{\default{17}\page{3}\revcontent{We can see that obtaining \acp {llr} needs the computation of various integrals evein in this simple case. Therefore, in general (e.g., with either multiple \acp {ap} or correlated shadowing/fading), the \ac {llr} can not be computed in closed-form, thus making \ac {np} test problematic.}\revsec{II}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {II-D}}Estimated Distance Approach}{3}}
\newlabel{seccomp}{{\unhbox \voidb@x \hbox {II-D}}{3}}
\newlabel{literature_1}{{18}{3}}
\zref@newlabel{literature_1}{\default{18}\page{3}\revcontent{We will compare our \ac {irlv} solutions with the \ac {eda} of {\cite {li2010security}}. In \ac {eda}, first the estimate $\mathaccentV {hat}05E{L}(\bm  {x}_{\rm  UD},\bm  {x}^{n}_{\rm  AP})$ of the \ac {ue}-\ac {ap} distance is obtained by inverting the path-loss formula, and then the \ac {ue} position is estimated as }\revsec{II}}
\citation{goodfellow}
\abx@aux@cite{goodfellow}
\abx@aux@segm{0}{0}{goodfellow}
\citation{bishop92}
\abx@aux@cite{bishop92}
\abx@aux@segm{0}{0}{bishop92}
\citation{bishop92}
\abx@aux@segm{0}{0}{bishop92}
\citation{Bishop2006}
\abx@aux@cite{Bishop2006}
\abx@aux@segm{0}{0}{Bishop2006}
\newlabel{probmindist}{{9}{4}}
\newlabel{literature_2}{{19}{4}}
\zref@newlabel{literature_2}{\default{19}\page{4}\revcontent{Let $\mathcal  B_0$ be the set of points of the border of $\mathcal  A_0$, and let the estimated distance of the \ac {ue} from the border $\mathcal  {B}_0$ $d_{\mathcal  B} = \qopname  \relax m{min}_{\bm  {x} \in \mathcal  {B}_0} \pm ||\mathaccentV {hat}05E{\bm  {x}}_{\rm  UD} - \bm  {x}||$, where the sign is negative if $\mathaccentV {hat}05E{\bm  {x}}_{\rm  UD} \in \mathcal  A_0$, and positive otherwise. Lastly, $d_{\mathcal  B}$ is compared with a suitable threshold $d_\delta $, chosen in order to achieve a desired FA probability, resulitng in $\mathaccentV {hat}05E{\mathcal  {H}}_{\rm  MMSE} = \mathcal  {H}_0, \hskip 1em\relax \text  {if } d_{\mathcal  B} < d_\delta $, $\mathaccentV {hat}05E{\mathcal  {H}}_{\rm  MMSE} = \mathcal  {H}_1$, otherwise.}\revsec{II}}
\newlabel{literature_3}{{20}{4}}
\zref@newlabel{literature_3}{\default{20}\page{4}\revcontent{Note that this approach requires the knowledge of the path-loss model (including knowledge of LOS and non-LOS state), which is quite unrealistic. Moreover, the estimator (9\hbox {}) is not optimal, since the position error is usually not a Gaussian variable.}\revsec{II}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {III}\Ac {irlv} by Machine Learning Approaches}{4}}
\newlabel{sec:irlvML}{{III}{4}}
\newlabel{supervised}{{21}{4}}
\zref@newlabel{supervised}{\default{21}\page{4}\revcontent{Therefore, we propose to use a \emph  {supervised} \ac {ml} approach}\revsec{III}}
\newlabel{framework2}{{22}{4}}
\zref@newlabel{framework2}{\default{22}\page{4}\revcontent{We stress the fact that the channel model of Section\nobreakspace  {}\unhbox \voidb@x \hbox {II-A}\hbox {} provides a realistic communication scenario, while the analysis that follows is general, as no specific channel statistics are assumed.}\revsec{III}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {III-A}}Neural Networks}{4}}
\newlabel{sec:nn}{{\unhbox \voidb@x \hbox {III-A}}{4}}
\newlabel{hyper1}{{23}{4}}
\zref@newlabel{hyper1}{\default{23}\page{4}\revcontent{Activation functions are typically chosen before training, while vectors $\bm  {w}_n^{(\ell )}$ are adapted according to the \ac {nn} learning algorithm in order to minimize the loss function.}\revsec{III}}
\newlabel{testfunNN}{{10}{4}}
\newlabel{lambdaNN}{{24}{4}}
\zref@newlabel{lambdaNN}{\default{24}\page{4}\revcontent{Parameter $\lambda $ shall be chosen in order to obtain the required \ac {fa} probability. The value of $\lambda $ which guarantees a certain \ac {fa} probability can obtained by simulation, whereas it can not be obtained by inverting the \ac {fa} probability function. This is due to the fact that the \ac {ml} framework is applied when there is no knowledge of the distribution of the variables and hence we can not compute a closed-form expression of the \ac {fa} probability}\revsec{III}}
\newlabel{ceNeeded}{{25}{4}}
\zref@newlabel{ceNeeded}{\default{25}\page{4}\revcontent{Based on the loss function to be optimized during training \acp {nn} can solve different problems, and we consider here two widely used loss functions: \ac {mse} and \ac {ce}. }\revsec{III}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {III-B}}\ac {nn} MSE Design}{4}}
\newlabel{sec: mse_train}{{\unhbox \voidb@x \hbox {III-B}}{4}}
\newlabel{ceNeeded2}{{26}{4}}
\zref@newlabel{ceNeeded2}{\default{26}\page{4}\revcontent{As optimal hypothesis testing is implemented via the \ac {np} framework, which exploits the knowledge of the \ac {llr} function, we aim at learning this function from data. This problem is referred to as \emph  {curve fitting} and it can be solved by training a \ac {nn} via the \ac {mse} loss function \cite {bishop92}.}\revsec{III}}
\newlabel{eq:mseFunct}{{11}{4}}
\citation{Ruck-90}
\abx@aux@cite{Ruck-90}
\abx@aux@segm{0}{0}{Ruck-90}
\citation{Ruck-90}
\abx@aux@segm{0}{0}{Ruck-90}
\citation{Bishop2006}
\abx@aux@segm{0}{0}{Bishop2006}
\citation{Bishop2006}
\abx@aux@segm{0}{0}{Bishop2006}
\citation{nostro}
\abx@aux@cite{nostro}
\abx@aux@segm{0}{0}{nostro}
\citation{Bishop2006}
\abx@aux@segm{0}{0}{Bishop2006}
\citation{Bishop2006}
\abx@aux@segm{0}{0}{Bishop2006}
\citation{Suykens1999}
\abx@aux@cite{Suykens1999}
\abx@aux@segm{0}{0}{Suykens1999}
\citation{guo2008novel}
\abx@aux@cite{guo2008novel}
\abx@aux@segm{0}{0}{guo2008novel}
\citation{Yevs}
\abx@aux@cite{Yevs}
\abx@aux@segm{0}{0}{Yevs}
\newlabel{eq:bayesDisc}{{12}{5}}
\newlabel{teoruck}{{1}{5}}
\newlabel{teoruck2}{{27}{5}}
\zref@newlabel{teoruck2}{\default{27}\page{5}\revcontent{Hence, Theorem 1\hbox {} proves that in the presence of i) perfect training, ii) an infinite number of neurons, and iii) convergence of the learning algorithm to the minimum error, the function implemented by \ac {mlp} is the Bayes optimal discriminant function. Now we have the following corollary.}\revsec{III}}
\newlabel{th:nn_np}{{1}{5}}
\newlabel{rev11b}{{28}{5}}
\zref@newlabel{rev11b}{\default{28}\page{5}\revcontent{Note that this result is quite general and can be applied to NNs with any number of layers and neurons, and any parameter adaptation approach, as long as the target design function is the MSE. Thus, Corollary 1 is suited also to describe the asymptotic behaviour of elaborate solutions, such as deep learning NNs.}\revsec{III}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {III-C}}\ac {nn} CE Design}{5}}
\newlabel{sec: ce_train}{{\unhbox \voidb@x \hbox {III-C}}{5}}
\newlabel{ceNeeded3}{{29}{5}}
\zref@newlabel{ceNeeded3}{\default{29}\page{5}\revcontent{Binary classification aims at assigning labels $0$ or $1$ to input vectors. In this case, the usual choice for the loss function is the \ac {ce} between the \ac {nn} output and the true labels of the input vector }\revsec{III}}
\newlabel{eq:ce}{{15}{5}}
\newlabel{th:nn_np2}{{2}{5}}
\newlabel{lasteq}{{17}{5}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {III-D}}Support Vector Machines}{5}}
\newlabel{sec:svm}{{\unhbox \voidb@x \hbox {III-D}}{5}}
\newlabel{eq:svm}{{18}{5}}
\newlabel{hyper2}{{30}{5}}
\zref@newlabel{hyper2}{\default{30}\page{5}\revcontent{While the feature-space transformation function is chosen before training \unhbox \voidb@x \hbox {\cite [Chapter\nobreakspace  {}7]{Bishop2006}}, vector $\bm  {w}$ must be properly learned from the data to obtain the desired hypothesis testing.}\revsec{III}}
\newlabel{eq:lssvm}{{19}{5}}
\newlabel{eq:lssvmOrig}{{19a}{5}}
\newlabel{eq:stpart}{{19b}{5}}
\newlabel{hyper3}{{31}{5}}
\zref@newlabel{hyper3}{\default{31}\page{5}\revcontent{where $C$ is not optimized by the learning algorithm, but must be tuned on training data using a separate procedure, e.g., see \cite {guo2008novel}.}\revsec{III}}
\newlabel{eq:els}{{20}{5}}
\citation{Ruck-90}
\abx@aux@segm{0}{0}{Ruck-90}
\citation{Bianchini2014}
\abx@aux@cite{Bianchini2014}
\abx@aux@segm{0}{0}{Bianchini2014}
\citation{vanGestel2004}
\abx@aux@cite{vanGestel2004}
\abx@aux@segm{0}{0}{vanGestel2004}
\citation{Kay-book}
\abx@aux@segm{0}{0}{Kay-book}
\citation{Hinton-2006}
\abx@aux@cite{Hinton-2006}
\abx@aux@segm{0}{0}{Hinton-2006}
\citation{goodfellow}
\abx@aux@segm{0}{0}{goodfellow}
\citation{goodfellow}
\abx@aux@segm{0}{0}{goodfellow}
\newlabel{lem:lem1}{{1}{6}}
\newlabel{th:lsnp}{{3}{6}}
\newlabel{revGO}{{32}{6}}
\zref@newlabel{revGO}{\default{32}\page{6}\revcontent{Consider a \ac {lssvm} with training converged to the global minimum of $\omega (\bm  {w},b)$, and using an infinite number of training points $\bm  {a}^{(i)}$ drawn from the finite alphabet $\mathcal  C$.}\revsec{III}}
\newlabel{eq:lssvmDim1}{{21}{6}}
\newlabel{eq:lsInf}{{22}{6}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {III-E}}Computational Costs of \ac {ml} Approaches}{6}}
\newlabel{sec:comp}{{\unhbox \voidb@x \hbox {III-E}}{6}}
\newlabel{comp1}{{33}{6}}
\zref@newlabel{comp1}{\default{33}\page{6}\revcontent{In this section, we briefly review the computational cost for i) training each machine and ii) making a prediction on a new data point. Let $\eta $ be the number of epochs (how many times each training point is used) of a \ac {nn}.}\revsec{III}}
\newlabel{comp2}{{34}{6}}
\zref@newlabel{comp2}{\default{34}\page{6}\revcontent{For a basic fully connected feed-forward \ac {nn}, the backpropagation training algorithm is $\mathcal  O(\eta \cdot S \cdot N_L \cdot N_{\rm  AP}^3)$ when the number of neurons of each hidden layer is proportional to the input size, while the prediction of a new unseen data point is $\mathcal  O(N_L \cdot N_{\rm  AP}^3)$. For a more detailed analysis, which takes into account also the cost of the choice of the activation function, see {\cite {Bianchini2014}}.}\revsec{III}}
\newlabel{comp3}{{35}{6}}
\zref@newlabel{comp3}{\default{35}\page{6}\revcontent{For a \ac {lssvm}, the estimate of the vector $\bm  {w}$ at training time is found by solving a linear set of equations (instead of the traditional quadratic programming of \ac {svm}). In general, the computational cost is $\mathcal  O(S^3)$; however, there are more efficient solutions that reduce this complexity to $\mathcal  O(S^2)$ (see \cite {vanGestel2004}). At test time, the prediction is linear in the number of features and the number of training points, i.e., $\mathcal  O(N_{\rm  AP} \cdot S)$.}\revsec{III}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {IV}\ac {irlv} By One-class Classification}{6}}
\newlabel{sec:OneClass}{{IV}{6}}
\newlabel{revOC}{{36}{6}}
\zref@newlabel{revOC}{\default{36}\page{6}\revcontent{In practice, collecting training points from region ${\mathcal  A}_1$ may be difficult, since this region may be large and not necessarily well defined (being simply the complement of $\mathcal  A_0$).}\revsec{IV}}
\newlabel{oneClass}{{37}{6}}
\zref@newlabel{oneClass}{\default{37}\page{6}\revcontent{Therefore, during the training phase, we collect attenuation vectors only from inside $\mathcal  {A}_0$ and use them to train a \ac {ml} classifier to distinguish between vectors belonging to $\mathcal  {A}_0$ and $\mathcal  {A}_1$ in the testing phase.}\revsec{IV}}
\newlabel{eq:GLRT}{{23}{6}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {IV-A}}Auto Encoder \ac {nn}}{6}}
\newlabel{sec:auto}{{\unhbox \voidb@x \hbox {IV-A}}{6}}
\newlabel{deep ae}{{38}{6}}
\zref@newlabel{deep ae}{\default{38}\page{6}\revcontent{We consider the \ac {ae} {\cite {Hinton-2006}}, i.e., a \ac {nn} trained to copy the input to the output. It comprises an encoder NN (with $N_e$ layers), which transforms the $N$-dimensional input data into the $M$-dimensional code, with $M<N$, and a decoder NN (with $N_d$ layers), which reconstructs the original high-dimensional data from the low-dimensional code. For an in-depth description of the \ac {ae} architecture, please refer to {\cite [Chapter 14]{goodfellow}}. When implementing \acp {ae}, it is convenient to use linear activation functions at the last hidden layer of the decoder {\cite [Chapter 14]{goodfellow}}.}\revsec{IV}}
\newlabel{eq: rec err}{{24}{6}}
\citation{choi2009}
\abx@aux@cite{choi2009}
\abx@aux@segm{0}{0}{choi2009}
\citation{Scholkopf2001estimating}
\abx@aux@cite{Scholkopf2001estimating}
\abx@aux@segm{0}{0}{Scholkopf2001estimating}
\citation{Bishop2006}
\abx@aux@segm{0}{0}{Bishop2006}
\citation{goodfellow}
\abx@aux@segm{0}{0}{goodfellow}
\newlabel{mseThresholding}{{39}{7}}
\zref@newlabel{mseThresholding}{\default{39}\page{7}\revcontent{As the \ac {ae} attempts to copy the input to its output, in the testing phase only vectors with features similar to those of the training set will be reconstructed with smaller \ac {mse}, whereas input vectors with different features will be mapped to different vectors at the output, with large \ac {mse}. Since training is based on vectors collected from area $\mathcal  {A}_0$ and we want to verify users located inside $\mathcal  {A}_0$, by thresholding the \ac {mse}, we can obtain the desired classifier.}\revsec{IV}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {IV-B}}One-Class LS-SVM}{7}}
\newlabel{eq:oneClassSvm}{{26}{7}}
\newlabel{eq:oneClass1}{{26a}{7}}
\newlabel{eq:oneClassConstr}{{26b}{7}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {V}Numerical Results}{7}}
\newlabel{sec:numRes}{{V}{7}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces \acs {roc} of \acp {irlv} methods for \ac {los}, uncorrelated fading/shadowing and various values of $\nu $ and $\sigma _{s, {\rm  dB}}$. Environment of Section \unhbox \voidb@x \hbox {II-C}\hbox {}.}}{7}}
\newlabel{fig:ceVSnp}{{2}{7}}
\newlabel{activation}{{40}{7}}
\zref@newlabel{activation}{\default{40}\page{7}\revcontent{For the \ac {nn} approach we use fully connected networks. For the two-class classification problem, the activation function of the input layer is the identity function, while the activation function of neurons in the hidden and output layers is the sigmoid {\cite [Section 6.2.2.2]{goodfellow}.}}\revsec{V}}
\newlabel{numResSimplScen3}{{41}{7}}
\zref@newlabel{numResSimplScen3}{\default{41}\page{7}\revcontent{\acp {nn} have been trained only for \ac {ce} loss function, as we have shown in Corollary 1 and Theorem 2 that with both \ac {mse} and \ac {ce} loss functions we achieve the same performance of the \ac {np} test.}\revsec{V}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {V-A}}Two-class \ac {irlv} With Single \ac {ap}}{7}}
\newlabel{sec:singleAp}{{\unhbox \voidb@x \hbox {V-A}}{7}}
\newlabel{numResSimplScen}{{42}{7}}
\zref@newlabel{numResSimplScen}{\default{42}\page{7}\revcontent{Firstly, we consider the environment of Section \unhbox \voidb@x \hbox {II-C}\hbox {} describing a small area. The channel model includes spatially uncorrelated fading or shadowing, with $R_{\rm  out}= 10$\nobreakspace  {}m, $R_{\rm  in} = 2$\nobreakspace  {}m, and $R_{\rm  min} = 0.1$\nobreakspace  {}m. Moreover, \ac {los} is assumed for path-loss. For uncorrelated fading, we consider two path-loss coefficients, namely $\nu =2$ and $3$; the closed-form expression of the \acp {llr} for the \ac {np} test are given by (28\hbox {}) and (30\hbox {}). With spatially uncorrelated shadowing, we set $\nu =2$, and three values of shadowing standard deviation, namely $\sigma _{s, {\rm  dB}} = 0.1$\nobreakspace  {}dB, $1.8$\nobreakspace  {}dB, and 6\nobreakspace  {}dB; the closed-form expression of the \acp {llr} for the \ac {np} test is given by (32\hbox {}). For the \ac {ml} approaches, we consider $S=10^5$ training points and a \ac {nn} with $N_L = 2$ hidden layers, each layer with $N^{(i)}=5$ neurons in layer $i = 1,2$.}\revsec{V}}
\newlabel{numResSimplScen2}{{43}{7}}
\zref@newlabel{numResSimplScen2}{\default{43}\page{7}\revcontent{Fig. 2\hbox {} shows the \ac {fa} probability versus the \ac {md} probability i.e., the \acf {roc}, obtained with the \ac {np} test, the \ac {nn}, and \ac {lssvm} classifiers. We notice that all models achieve the same performance, confirming our theoretical results that both \ac {nn} and \ac {lssvm} with sufficient training data and number of hidden layers are optimal as \ac {np}.}\revsec{V}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Reference environment.}}{8}}
\newlabel{fig:mBS}{{3}{8}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Example of attenuation map including path-loss and shadowing, with the \ac {ap} positioned at the center.}}{8}}
\newlabel{fig:map}{{4}{8}}
\newlabel{numResSimplScen4}{{44}{8}}
\zref@newlabel{numResSimplScen4}{\default{44}\page{8}\revcontent{We observe that fading has more impact on the performance than shadowing, yielding higher \ac {fa} and \ac {md} probabilities. Still, with fading, a higher path-loss coefficient provides better results, since the attenuation increases more with the distance, thus easing classification. For spatially uncorrelated shadowing, performance improves as $\sigma _{s, {\rm  dB}}$ decreases, since in this case path-loss alone already provides error-free decisions, thus the shadowing component is a disturbance in the decision process.}\revsec{V}}
\newlabel{building}{{45}{8}}
\zref@newlabel{building}{\default{45}\page{8}\revcontent{The \ac {roi} is inside the south-west building, modelling for example a scenario wherein privileged network resources are accessible only to users inside an office.}\revsec{V}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces \ac {roc} of \ac {irlv} methods, with a \ac {nn} having $N_L=1$ and two values of $N_h$. Environment of Fig. 3\hbox {}, with one \ac {ap} located at the street intersection, $d_1 = 50$\nobreakspace  {}m, $d_2 = 50$\nobreakspace  {}m, $\beta _1 = \beta _2 = 150$ and correlated shadowing ($\sigma _{s, {\rm  dB}} = 8$\nobreakspace  {}dB).}}{8}}
\newlabel{fig:trueMap}{{5}{8}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {V-B}}Two-class \ac {irlv} With Multiple \acp {ap}}{8}}
\newlabel{sec:res_fading}{{\unhbox \voidb@x \hbox {V-B}}{8}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {paragraph}{\numberline {\unhbox \voidb@x \hbox {V-B}0a}No fading average}{8}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces \ac {roc} of \ac {irlv} methods for different values of training-set size $S$. Environment of Fig. 3\hbox {}, with $N_{\rm  AP}=10$, $d_1 = 50$\nobreakspace  {}m, $d_2 = 50$, $\beta _1 = \beta _2 = 150$\nobreakspace  {}m, and $\sigma _{s,{\rm  dB}} = 8$\nobreakspace  {}dB.}}{9}}
\newlabel{fig:kf1}{{6}{9}}
\newlabel{revnewarea}{{46}{9}}
\zref@newlabel{revnewarea}{\default{46}\page{9}\revcontent{We have also considered a different \ac {roi} layout, with $d_1 = 100$\nobreakspace  {}m, $d_2 = 255$\nobreakspace  {}m and $\beta _1 = \beta _2 = 150$\nobreakspace  {}m. The \ac {roi} is still positioned in the south-west corner, but it includes both the crossroads and $\rm  AP_8$ (see Fig. 3\hbox {}). Channel parameters are the same of Fig. 6\hbox {}.}\revsec{V}}
\newlabel{newarea2}{{47}{9}}
\zref@newlabel{newarea2}{\default{47}\page{9}\revcontent{Fig. 7\hbox {} shows the resulting \ac {roc}, still obtained by averaging the \ac {md} probabilities over the shadowing maps. Including the street inside the \ac {roi}, with its \ac {los} path-loss, turns out to facilitate \ac {irlv} resulting in lower \ac {fa} and \ac {md} probabilities.}\revsec{V}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {paragraph}{\numberline {\unhbox \voidb@x \hbox {V-B}0b}Effect of fading average}{9}}
\newlabel{revLI2a}{{48}{9}}
\zref@newlabel{revLI2a}{\default{48}\page{9}\revcontent{We also report the performance of \ac {eda}, assuming to know the path-loss relation between the attenuation and the distance.}\revsec{V}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces \ac {roc} of \ac {irlv} methods for different values of training-set size $S$. Environment of Fig. 3\hbox {}, with $N_{\rm  AP}=10$, $d_1 = 100$\nobreakspace  {}m, $d_2 = 225$\nobreakspace  {}m, $\beta _1 = \beta _2 = 150$, and $\sigma _{s,{\rm  dB}} = 8$\nobreakspace  {}dB.}}{9}}
\newlabel{fig:kf1_newArea}{{7}{9}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces \ac {roc} of \ac {irlv} methods for different averages of fading. Environment of Fig. 3\hbox {}, with $N_{\rm  AP}=5$, $d_1 = 50 $\nobreakspace  {}m, $d_2 = 50$\nobreakspace  {}m, $\beta _1 = \beta _2 = 150$\nobreakspace  {}m, and $\sigma _{s,{\rm  dB}} = 8$\nobreakspace  {}dB.}}{9}}
\newlabel{fig:kf10-5}{{8}{9}}
\newlabel{rev2fad}{{49}{9}}
\zref@newlabel{rev2fad}{\default{49}\page{9}\revcontent{We note that both \ac {md} and \ac {fa} probabilities can be significantly reduced by averaging fading, thus approaching the performance on channels without fading. Indeed, an average of 10 fading realizations already reduces the average \ac {md} probability from $10^{-1}$ to $10^{-2}$, for an \ac {fa} probability of $2\cdot 10^{-1}$, while we achieve an average \ac {md} probability of $4\cdot 10^{-4}$ without fading using a \ac {nn}. We also notice that, in absence of fading, \ac {svm} significantly outperforms \ac {nn} even if \ac {nn} uses a larger $S$. This suggests that, in this scenario, the \ac {nn} has not yet converged to the optimum, wherein potentially very good performance can be achieved, due to limits in architecture, computational capabilities, and design algorithms. We should remember, in fact, that the number of parameters defining the \ac {svm} grows with the training size, while the number of parameters of the \ac {nn} is set a-priori.}\revsec{V}}
\newlabel{revLI2b}{{50}{9}}
\zref@newlabel{revLI2b}{\default{50}\page{9}\revcontent{Lastly, we observe that the proposed \ac {ml} techniques (both with and without fading) outperform \ac {eda}, whose performance has been obtained on channels without fading. This is due to the fact that \ac {eda} is more severely affected by shadowing seen as disturbance in the derivation of the distance, while \ac {ml} solutions may exploit it in making the decision, while still not relying on specific channel models.}\revsec{V}}
\citation{MOMENTUM-D53}
\abx@aux@cite{MOMENTUM-D53}
\abx@aux@segm{0}{0}{MOMENTUM-D53}
\citation{Hinton-2006}
\abx@aux@segm{0}{0}{Hinton-2006}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces \ac {roc} of \ac {irlv} methods for the experimental data.}}{10}}
\newlabel{fig:Berlinnew}{{9}{10}}
\newlabel{Berlin}{{51}{10}}
\zref@newlabel{Berlin}{\default{51}\page{10}\revcontent{We have tested the proposed \ac {irlv} solutions on real data collected by the MOMENTUM project {\cite {MOMENTUM-D53}} in a measurement campaign at Alexanderplatz in Berlin (Germany). Attenuations at the frequency of the \ac {gsm} (that may refer to a cellular IoT scenario in our \ac {irlv} context) have been measured for several \acp {ap} in an area of $4500\nobreakspace  {}{\rm  m} \cdot 4500\nobreakspace  {}{\rm  m}$, on a measurement grid of $50 \cdot 50$\nobreakspace  {}m. We have considered 10 attenuation maps, corresponding to 10 \ac {ap} positions (all in meters) $\bm  {x}_{\rm  AP}^{(1)} = [2500, 2500]$, $\bm  {x}_{\rm  AP}^{(2)} = [500, 4000]$, $\bm  {x}_{\rm  AP}^{(3)} = [4000, 4000]$, $\bm  {x}_{\rm  AP}^{(4)} = [500, 500]$, $\bm  {x}_{\rm  AP}^{(5)} = [4000, 500]$, $\bm  {x}_{\rm  AP}^{(6)} = [100, 4500]$, $\bm  {x}_{\rm  AP}^{(7)} = [1000, 400]$, $\bm  {x}_{\rm  AP}^{(8)} = [4000, 500]$, $\bm  {x}_{\rm  AP}^{(9)} = [4300, 4000]$, and $\bm  {x}_{\rm  AP}^{(10)} = [4500, 500]$. The \ac {roi} has been positioned in the lower-right corner, corresponding to, following the same notation of Fig 3\hbox {}, $d_1 = 3000$\nobreakspace  {}m, $ d_2 = 1500$\nobreakspace  {}m, and $\beta _1 = \beta _2 = 1000 $\nobreakspace  {}m. In this case, we have a single realization of any channel effect (path-loss, shadowing, fading, \ldots  ) per location, for a total of 8464 realizations, 5000 of which have been used for training and the rest for testing. For \ac {nn}, we set $L = 3$ and $N^{(i)} = 500$, $i = 1,2,3$. Fig. 9\hbox {} shows the \ac {roc} for both \ac {nn} and \ac {lssvm}. The performance is in line with the other figures obtained by simulation. Still, due to the small size of the available training set, \acp {roc} are not smooth. Moreover, we notice that \ac {svm} and \ac {nn} achieve approximately the same performance.}\revsec{V}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {paragraph}{\numberline {\unhbox \voidb@x \hbox {V-B}0c}Results on experimental data}{10}}
\newlabel{revLI}{{52}{10}}
\zref@newlabel{revLI}{\default{52}\page{10}\revcontent{Note also that, in order to use \ac {eda}, we should first know the path-loss to convert the attenuation estimates into distances, an information not immediately available from the experimental data. Therefore we could not compare \ac {ml} with \ac {eda} in this case, further demonstrating the utility of \ac {ml} model-less techniques for \ac {irlv}.}\revsec{V}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces \ac {roc} for one-class \acp {irlv} for different training-set sizes. Environment of Fig 3\hbox {} with $N_{\rm  AP}=10$, $k_f=1$, and \ac {ae} with $N_L = 7$. }}{10}}
\newlabel{fig:kf1Oc}{{10}{10}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {V-C}}One-Class \ac {irlv} With Multiple \acp {ap}}{10}}
\newlabel{sec:numResOneClass}{{\unhbox \voidb@x \hbox {V-C}}{10}}
\newlabel{designAE}{{53}{10}}
\zref@newlabel{designAE}{\default{53}\page{10}\revcontent{The \ac {ae} has been designed according to \cite {Hinton-2006}, i.e., all neurons use the logistic sigmoid as activation function except for those in the central hidden layer, using linear activation functions. The \ac {ae} has $N_L = 7$ hidden layers with 7, 6, 3, 2, 3, 6, and 7 neurons, respectively. Weights are initialized randomly.}\revsec{V}}
\newlabel{fadingRes}{{54}{10}}
\zref@newlabel{fadingRes}{\default{54}\page{10}\revcontent{Here, we consider the effects of fading and the choice of the number of training points $S$. Fig. 10\hbox {} shows the \ac {roc} for one-class \ac {irlv} systems for $k_f = 1$ and two values of $S$. We first notice that both \ac {ae} and \ac {oclssvm} converge for $S = 5 \cdot 10^{3}$, and the \ac {svm}-based solution outperforms the \ac {nn}-based solution, as already seen in the case of two-class classification. Fig. 11\hbox {} shows the \ac {roc} for $k_f=1$ and 10, while $n_x=2 \cdot 10^4$. We note that, for both \ac {ml} techniques, averaging over fading significantly improves the performance.}\revsec{V}}
\newlabel{revLI3}{{55}{10}}
\zref@newlabel{revLI3}{\default{55}\page{10}\revcontent{We also report the performance of \ac {eda} obtained without fading and assuming the knowledge of the path-loss relation between attenuation and distance. Again, we note that the proposed \ac {ml} techniques significantly outperform \ac {eda} (in the absence of fading).}\revsec{V}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {VI}Conclusions}{10}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces \ac {roc} of one-class \acp {irlv} for different values of $k_f$. Environment of Fig. 3\hbox {} with $N_{\rm  AP}=10$, and $n_x= 2 \cdot 10^4$, \ac {ae} with $N_L = 7$. }}{11}}
\newlabel{fig:kf10Oc}{{11}{11}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{Appendix\nobreakspace  A: LLRs derivation}{11}}
\newlabel{sec:llrDer}{{A}{11}}
\newlabel{simpleScen3}{{56}{11}}
\zref@newlabel{simpleScen3}{\default{56}\page{11}\revcontent{Assuming spatially uncorrelated Rayleigh fading, without shadowing (i.e., $\sigma _{s,\rm  dB}=0)$, given a \ac {ue} located at distance $d$, the channel gain $g=1/a$ is exponentially distributed with mean (in dB) $P_{\rm  PL,LOS}(d)$ given by $\textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces 3\hbox {}\unskip \@@italiccorr )}}$. Letting}\revsec{A}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsubsection}{\numberline {\unhbox \voidb@x \hbox {A-}1}Uncorrelated Fading scenario}{11}}
\newlabel{simpleScen3_1}{{57}{11}}
\zref@newlabel{simpleScen3_1}{\default{57}\page{11}\revcontent{from the uniform \ac {ue} distribution and \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces 8\hbox {}\unskip \@@italiccorr )}} we have $p(a|\mathcal  {H}_0)=F(\Delta _0,R_{\rm  min},R_{\rm  in})$, whereas $p(a|\mathcal  {H}_1) = F(\Delta _1,R_{\rm  in},R_{\rm  out})$.}\revsec{A}}
\newlabel{simpleScen3_2}{{58}{11}}
\zref@newlabel{simpleScen3_2}{\default{58}\page{11}\revcontent{By computing integrals for path-loss coefficient $\nu = 2$, the \ac {llr} is}\revsec{A}}
\newlabel{eq:llr1}{{28}{11}}
\newlabel{simpleScen3_3}{{59}{11}}
\zref@newlabel{simpleScen3_3}{\default{59}\page{11}\revcontent{Let $\Gamma (\gamma ,b)= \DOTSI \intop \ilimits@ _{b}^{\infty }t^{\gamma -1}e^{-t} dt$ be the incomplete gamma function, then for $\nu =3$ we have instead}\revsec{A}}
\newlabel{eq:llr2}{{30}{11}}
\newlabel{simpleScen4}{{60}{11}}
\zref@newlabel{simpleScen4}{\default{60}\page{11}\revcontent{Assuming spatially uncorrelated shadowing, without fading we have $10\qopname  \relax o{log}_{10}a^{(n)}=P^{(n)}_{\rm  PL}+s$, i.e., the received power from a given location is distributed in the logarithmic domain as a Gaussian random variable with mean value given by the path-loss (3\hbox {}) and standard deviation $\sigma _{s,\rm  dB}$. Letting}\revsec{A}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsubsection}{\numberline {\unhbox \voidb@x \hbox {A-}2}Uncorrelated shadowing scenario}{11}}
\newlabel{eq:sh}{{31}{11}}
\newlabel{simpleScen4_1}{{61}{11}}
\zref@newlabel{simpleScen4_1}{\default{61}\page{11}\revcontent{from (8\hbox {}), the \ac {pdf} of incurring an attenuation $a$ in hypothesis $\mathcal  {H}_0$ is $p(a|\mathcal  {H}_0)=G(\Delta _0,R_{\rm  min}, R_{\rm  in})$, and $p(a|\mathcal  {H}_1) = G(\Delta _1, R_{\rm  in},R_{\rm  out})$.}\revsec{A}}
\newlabel{simpleScen4_2}{{62}{11}}
\zref@newlabel{simpleScen4_2}{\default{62}\page{11}\revcontent{By solving the integral in (31\hbox {}) we obtain the \ac {llr}}\revsec{A}}
\newlabel{eq:llr3}{{32}{11}}
\newlabel{simpleScen4_3}{{63}{11}}
\zref@newlabel{simpleScen4_3}{\default{63}\page{11}\revcontent{where $\erf  (x)= \frac  {2}{\sqrt  {\pi }}\DOTSI \intop \ilimits@ _0^x e^{-t^2} {\tt  d}t$ is the error function and}\revsec{A}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{Appendix\nobreakspace  B: Proof of Theorem 3}{11}}
\newlabel{sec:proofTh3}{{B}{11}}
\newlabel{eq:lssvm2}{{34}{12}}
\newlabel{eq:system1}{{36}{12}}
\newlabel{eq:wSolution}{{37}{12}}
\abx@aux@refcontextdefaultsdone
\abx@aux@defaultrefcontext{0}{Zeng-survey}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{8376254}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{wei2013}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{7903611}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{quaglia}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{li2010security}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{7270404}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{Baracca-12}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{7398138}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{Brands}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{singelee2005location}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{song2008secure}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{Sastry}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{Vora}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{7145434}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{yan2016location}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{Cover-book}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{xiao-2018}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{tian2015robust}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{3gpp}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{goldsmith2005}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{Kay-book}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{goodfellow}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{bishop92}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{Bishop2006}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{Ruck-90}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{nostro}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{Suykens1999}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{guo2008novel}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{Yevs}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{Bianchini2014}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{vanGestel2004}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{Hinton-2006}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{choi2009}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{Scholkopf2001estimating}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{MOMENTUM-D53}{none/global//global/global}
